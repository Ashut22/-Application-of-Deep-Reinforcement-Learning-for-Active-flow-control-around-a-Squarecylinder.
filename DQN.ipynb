{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, in_states, h1_nodes, out_actions):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_states, h1_nodes)\n",
    "        self.out = nn.Linear(h1_nodes, out_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "class ReplayMemory():\n",
    "    def __init__(self, maxlen):\n",
    "        self.memory = deque([], maxlen=maxlen)\n",
    "\n",
    "    def append(self, transition):\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, sample_size):\n",
    "        return random.sample(self.memory, sample_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class MountainCarDQL():\n",
    "    learning_rate_a = 0.01\n",
    "    discount_factor_g = 0.9\n",
    "    network_sync_rate = 50000\n",
    "    replay_memory_size = 100000\n",
    "    mini_batch_size = 32\n",
    "    num_divisions = 20\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = None\n",
    "\n",
    "    def train(self, episodes, render=False):\n",
    "        env = gym.make('MountainCar-v0', render_mode='human' if render else None)\n",
    "        num_states = env.observation_space.shape[0]\n",
    "        num_actions = env.action_space.n\n",
    "        self.pos_space = np.linspace(env.observation_space.low[0], env.observation_space.high[0], self.num_divisions)\n",
    "        self.vel_space = np.linspace(env.observation_space.low[1], env.observation_space.high[1], self.num_divisions)\n",
    "        epsilon = 1\n",
    "        memory = ReplayMemory(self.replay_memory_size)\n",
    "        policy_dqn = DQN(in_states=num_states, h1_nodes=10, out_actions=num_actions)\n",
    "        target_dqn = DQN(in_states=num_states, h1_nodes=10, out_actions=num_actions)\n",
    "        target_dqn.load_state_dict(policy_dqn.state_dict())\n",
    "        self.optimizer = torch.optim.Adam(policy_dqn.parameters(), lr=self.learning_rate_a)\n",
    "        rewards_per_episode = []\n",
    "        epsilon_history = []\n",
    "        step_count = 0\n",
    "        goal_reached = False\n",
    "        best_rewards = -200\n",
    "        for i in range(episodes):\n",
    "            state = env.reset()[0]\n",
    "            terminated = False\n",
    "            rewards = 0\n",
    "            while not terminated and rewards > -1000:\n",
    "                if random.random() < epsilon:\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        action = policy_dqn(self.state_to_dqn_input(state)).argmax().item()\n",
    "                new_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                rewards += reward\n",
    "                memory.append((state, action, new_state, reward, terminated))\n",
    "                state = new_state\n",
    "                step_count += 1\n",
    "            rewards_per_episode.append(rewards)\n",
    "            if terminated:\n",
    "                goal_reached = True\n",
    "            if i != 0 and i % 1000 == 0:\n",
    "                print(f'Episode {i} Epsilon {epsilon}')\n",
    "                self.plot_progress(rewards_per_episode, epsilon_history)\n",
    "            if rewards > best_rewards:\n",
    "                best_rewards = rewards\n",
    "                print(f'Best rewards so far: {best_rewards}')\n",
    "                torch.save(policy_dqn.state_dict(), f\"mountaincar_dql_{i}.pt\")\n",
    "            if len(memory) > self.mini_batch_size and goal_reached:\n",
    "                mini_batch = memory.sample(self.mini_batch_size)\n",
    "                self.optimize(mini_batch, policy_dqn, target_dqn)\n",
    "                epsilon = max(epsilon - 1 / episodes, 0)\n",
    "                epsilon_history.append(epsilon)\n",
    "                if step_count > self.network_sync_rate:\n",
    "                    target_dqn.load_state_dict(policy_dqn.state_dict())\n",
    "                    step_count = 0\n",
    "        env.close()\n",
    "\n",
    "    def plot_progress(self, rewards_per_episode, epsilon_history):\n",
    "        plt.figure(1)\n",
    "        plt.subplot(121)\n",
    "        plt.plot(rewards_per_episode)\n",
    "        plt.subplot(122)\n",
    "        plt.plot(epsilon_history)\n",
    "        plt.savefig('mountaincar_dql.png')\n",
    "\n",
    "    def optimize(self, mini_batch, policy_dqn, target_dqn):\n",
    "        current_q_list = []\n",
    "        target_q_list = []\n",
    "        for state, action, new_state, reward, terminated in mini_batch:\n",
    "            if terminated:\n",
    "                target = torch.FloatTensor([reward])\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    target = torch.FloatTensor(\n",
    "                        reward + self.discount_factor_g * target_dqn(self.state_to_dqn_input(new_state)).max()\n",
    "                    )\n",
    "            current_q = policy_dqn(self.state_to_dqn_input(state))\n",
    "            current_q_list.append(current_q)\n",
    "            target_q = target_dqn(self.state_to_dqn_input(state))\n",
    "            target_q[action] = target\n",
    "            target_q_list.append(target_q)\n",
    "        loss = self.loss_fn(torch.stack(current_q_list), torch.stack(target_q_list))\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def state_to_dqn_input(self, state) -> torch.Tensor:\n",
    "        state_p = np.digitize(state[0], self.pos_space)\n",
    "        state_v = np.digitize(state[1], self.vel_space)\n",
    "        return torch.FloatTensor([state_p, state_v])\n",
    "\n",
    "    def test(self, episodes, model_filepath):\n",
    "        env = gym.make('MountainCar-v0', render_mode='human')\n",
    "        num_states = env.observation_space.shape[0]\n",
    "        num_actions = env.action_space.n\n",
    "        self.pos_space = np.linspace(env.observation_space.low[0], env.observation_space.high[0], self.num_divisions)\n",
    "        self.vel_space = np.linspace(env.observation_space.low[1], env.observation_space.high[1], self.num_divisions)\n",
    "        policy_dqn = DQN(in_states=num_states, h1_nodes=10, out_actions=num_actions)\n",
    "        policy_dqn.load_state_dict(torch.load(model_filepath))\n",
    "        policy_dqn.eval()\n",
    "        for i in range(episodes):\n",
    "            state = env.reset()[0]\n",
    "            terminated = False\n",
    "            truncated = False\n",
    "            while not terminated and not truncated:\n",
    "                with torch.no_grad():\n",
    "                    action = policy_dqn(self.state_to_dqn_input(state)).argmax().item()\n",
    "                state, reward, terminated, truncated, _ = env.step(action)\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    mountaincar = MountainCarDQL()\n",
    "    mountaincar.train(5000, True)\n",
    "    mountaincar.test(10, \"mountaincar_dql_17000.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
